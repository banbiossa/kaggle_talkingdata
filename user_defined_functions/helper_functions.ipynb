{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-09T06:27:02.953046Z",
     "start_time": "2017-08-09T06:27:02.947514Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T06:00:59.324025Z",
     "start_time": "2017-08-23T06:00:59.318609Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def sizeof_fmt2(df):\n",
    "    suffix = 'B'\n",
    "    num = sys.getsizeof(df)\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save to aws\n",
    "- helper function for saving dataframes to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T09:26:14.044270Z",
     "start_time": "2017-08-10T09:26:13.592635Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import string, random\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "def save_df_to_s3(df, path_to_directory, compression='gzip', encoding = None):\n",
    "    # parse url\n",
    "    # the default is s3://awsant-lalaport-data\n",
    "    o = urlparse(path_to_directory)\n",
    "    scheme = 's3' if ((o.scheme == '') | (o.scheme == 's3n')) else o.scheme\n",
    "    mybucket = o.netloc if o.netloc != '' else 'awsant-lalaport-data'\n",
    "    # remove heading '/' if there is any\n",
    "    path_to_directory = re.sub('^/', '', o.path)\n",
    "    \n",
    "    # make conncection\n",
    "    s3 = boto3.resource(scheme)\n",
    "    bucket = s3.Bucket(mybucket)\n",
    "    \n",
    "    # where am I writing to ?\n",
    "    print(\"saving to s3://\" + mybucket + \"/\" + path_to_directory)\n",
    "    \n",
    "    # write in tempfile\n",
    "    tempfile = ''.join(random.choices(string.ascii_uppercase + string.digits, k=20)) + \".gz\"\n",
    "    df.to_csv(tempfile, compression = compression, encoding = encoding)\n",
    "    s3.Object(mybucket, path_to_directory).put(Body=open(tempfile, 'rb'))\n",
    "    \n",
    "    # erase tempfile\n",
    "    os.remove(tempfile)\n",
    "    \n",
    "    print(\"saved! to s3://\" + mybucket + \"/\" + path_to_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import string, random\n",
    "\n",
    "def save_df_to_s3_csv(df, path_to_directory):\n",
    "    # make conncection\n",
    "    s3 = boto3.resource('s3')\n",
    "    mybucket = 'awsant-lalaport-data'\n",
    "    bucket = s3.Bucket(mybucket)\n",
    "    \n",
    "    # where am I writing to ?\n",
    "    print(\"saving to s3://\" + mybucket + \"/\" + path_to_directory)\n",
    "    \n",
    "    # write in tempfile\n",
    "    tempfile = ''.join(random.choices(string.ascii_uppercase + string.digits, k=20)) + \".csv\"\n",
    "    df.to_csv(tempfile)\n",
    "    s3.Object(mybucket, path_to_directory).put(Body=open(tempfile, 'rb'))\n",
    "    \n",
    "    # erase tempfile\n",
    "    os.remove(tempfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import string, random\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def save_object_to_s3(object_to_save, path_to_directory):\n",
    "    # parse url\n",
    "    # the default is s3://awsant-lalaport-data\n",
    "    o = urlparse(path_to_directory)\n",
    "    scheme = 's3' if ((o.scheme == '') | (o.scheme == 's3n')) else o.scheme\n",
    "    mybucket = o.netloc if o.netloc != '' else 'awsant-lalaport-data'\n",
    "    path_to_directory =o.path[1:]\n",
    "    \n",
    "    # make conncection\n",
    "    s3 = boto3.resource(scheme)\n",
    "    bucket = s3.Bucket(mybucket)\n",
    "    \n",
    "    # where am I writing to ?\n",
    "    print(\"saving to s3://\" + mybucket + \"/\" + path_to_directory)\n",
    "    \n",
    "    # put file\n",
    "    s3.Object(mybucket, path_to_directory).put(Body=object_to_save)\n",
    "    \n",
    "    # erase file\n",
    "    os.remove(object_to_save)\n",
    "    \n",
    "    print(\"saved! to s3://\" + mybucket + \"/\" + path_to_directory)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T08:29:12.672919Z",
     "start_time": "2018-02-16T08:29:12.666925Z"
    }
   },
   "outputs": [],
   "source": [
    "import s3io\n",
    "from sklearn.externals import joblib\n",
    "from urllib.parse import urlparse\n",
    "import boto\n",
    "\n",
    "def joblib_dump_s3(object_to_save, path_to_directory):\n",
    "    # parse url\n",
    "    # the default is s3://awsant-lalaport-data\n",
    "    o = urlparse(path_to_directory)\n",
    "    scheme = 's3' if ((o.scheme == '') | (o.scheme == 's3n')) else o.scheme\n",
    "    bucket = o.netloc if o.netloc != '' else 'awsant-lalaport-data'\n",
    "    key = re.sub('^/', '', o.path)\n",
    "    \n",
    "    #s3 connection\n",
    "    s3 = boto.connect_s3()\n",
    "    \n",
    "    # open connection and save\n",
    "    with s3io.open('s3://{0}/{1}'.format(bucket, key), mode = 'w',  s3_connection=s3) as s3_file:\n",
    "        joblib.dump(object_to_save, s3_file, compress = True)\n",
    "    \n",
    "    print(\"saved to s3://\" + bucket + \"/\" + key) \n",
    "        \n",
    "def joblib_load_s3(path_to_directory):\n",
    "    # parse url\n",
    "    # the default is s3://awsant-lalaport-data\n",
    "    o = urlparse(path_to_directory)\n",
    "    scheme = 's3' if ((o.scheme == '') | (o.scheme == 's3n')) else o.scheme\n",
    "    bucket = 'awsant-lalaport-data' if o.netloc == '' else o.netloc\n",
    "    key = re.sub('^/', '', o.path)\n",
    "    \n",
    "    #s3 connection\n",
    "    s3 = boto.connect_s3()\n",
    "    \n",
    "    print('Loading from s3://' + bucket + '/' + key)\n",
    "    \n",
    "    with s3io.open('s3://{0}/{1}'.format(bucket, key), mode = 'r', s3_connection=s3) as s3_file:\n",
    "        return joblib.load(s3_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T09:23:52.134280Z",
     "start_time": "2017-08-30T09:23:52.130220Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_unique_counts(df):\n",
    "    for c in df.columns:\n",
    "        print (\"------------------ %s -----------------\" % c)\n",
    "        print (df[c].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-30T09:22:45.566719Z",
     "start_time": "2017-08-30T09:22:45.562517Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_value_counts(df, nrows = 15):\n",
    "    for c in df.columns:\n",
    "        print (\"------------------ %s -----------------\" % c)\n",
    "        print (df[c].value_counts().head(nrows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# filter SCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shopping around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-16T04:08:44.496658Z",
     "start_time": "2018-02-16T04:08:44.451599Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-05T01:21:24.484554Z",
     "start_time": "2017-09-05T01:21:24.461206Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import git\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "module_path = repo.working_tree_dir\n",
    "\n",
    "def filter_shopping_around_scs(df, module_path = module_path):\n",
    "\n",
    "    # get master\n",
    "    master_path = Path(module_path) / 'master/ms_sc.tab'\n",
    "    master = pd.read_table(master_path)\n",
    "    master = master[pd.isnull(master.type) != True]\n",
    "\n",
    "    # get sc to use\n",
    "    ms_sc_path = Path(module_path) / 'master/MS_SC_v5.csv'\n",
    "    sc_to_use = pd.read_csv(ms_sc_path, skiprows=1, usecols=master.column)\n",
    "\n",
    "    # get those to use in shopping around\n",
    "    sc_to_use = sc_to_use[sc_to_use['ShoppingAround'] == 'Use']\n",
    "    \n",
    "    # change type\n",
    "    sc_to_use['SC_CODE'] = sc_to_use['SC_CODE'].astype('int64')\n",
    "    sc_to_use['SC_CODE_NEW'] = sc_to_use['SC_CODE_NEW'].astype('int64')\n",
    "    \n",
    "    # get SC code new\n",
    "    new_sc_codes = sc_to_use[['SC_CODE','SC_CODE_NEW']]\n",
    "    \n",
    "    # merge new sc codes\n",
    "    df = df.merge(new_sc_codes)\n",
    "    \n",
    "    # filter SC codes in those to use\n",
    "    df = df[df['SC_CODE_NEW'].isin(new_sc_codes['SC_CODE_NEW'])]\n",
    "    \n",
    "    # drop old sc\n",
    "    del df['SC_CODE']\n",
    "    \n",
    "    # rename sc codes\n",
    "    df.rename(index=str, columns = {'SC_CODE_NEW':'SC_CODE'}, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-21T10:50:23.516743Z",
     "start_time": "2017-09-21T10:50:23.496397Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import git\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "module_path = repo.working_tree_dir\n",
    "\n",
    "def filter_campaign_scs(df, module_path):\n",
    "    # delete new sc codes\n",
    "    if 'SC_CODE_NEW' in df:\n",
    "        del df['SC_CODE_NEW']\n",
    "\n",
    "    # get sc to use\n",
    "    ms_sc_path = Path(module_path) / 'master/MS_SC_v7.xlsx'\n",
    "    sc_to_use = pd.read_excel(ms_sc_path, skiprows=1)\n",
    "\n",
    "    # get those to use in shopping around\n",
    "    sc_to_use = sc_to_use.loc[sc_to_use['CampaignAnalysis'] == 'Use',:]\n",
    "    \n",
    "    # change type\n",
    "    sc_to_use['SC_CODE'] = sc_to_use['SC_CODE'].astype('int64')\n",
    "    sc_to_use['SC_CODE_NEW'] = sc_to_use['SC_CODE_NEW'].astype('int64')\n",
    "    \n",
    "    # get SC code new\n",
    "    new_sc_codes = sc_to_use.loc[:,['SC_CODE','SC_CODE_NEW']]\n",
    "    \n",
    "    # merge new sc codes\n",
    "    df = df.merge(new_sc_codes)\n",
    "    \n",
    "    # filter SC codes in those to use\n",
    "    df = df.loc[df['SC_CODE_NEW'].isin(new_sc_codes['SC_CODE_NEW']),:]\n",
    "    \n",
    "    # drop old sc\n",
    "    del df['SC_CODE']\n",
    "    \n",
    "    # rename sc codes\n",
    "    df.rename(index=str, columns = {'SC_CODE_NEW':'SC_CODE'}, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# lift chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-11T09:53:26.813953Z",
     "start_time": "2017-09-11T09:53:26.789418Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def print_lift_chart(actual, probability, title, num_bins = 10, \n",
    "                     save_file = False, module_path = None, file_name = None,\n",
    "                     save_summary = False, summary_save_path = None):\n",
    "    # make data\n",
    "    d = {'actual':actual, 'probability':probability}\n",
    "    df = pd.DataFrame(d)\n",
    "    \n",
    "    # get mean probability\n",
    "    mean_probability = df.probability.agg('mean')\n",
    "    mean_actual = df.actual.agg('mean')\n",
    "    \n",
    "    # add bin\n",
    "    df['bins'] = pd.qcut(df.probability, num_bins, duplicates='drop')\n",
    "    \n",
    "    # summarize on bin\n",
    "    summary = df.groupby('bins').agg('mean')\n",
    "    summary['Bins'] = range(1,len(summary.index)+1)\n",
    "    \n",
    "    # plot\n",
    "    ax = sns.pointplot(x = 'Bins', y = 'actual', data = summary,\n",
    "                  color = 'grey')\n",
    "\n",
    "    # add title\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # add mean\n",
    "    ax.plot([0,num_bins],[mean_actual,mean_actual], linewidth=2)\n",
    "    \n",
    "    # set ylim\n",
    "    ax.set(ylim=(0, None))\n",
    "    \n",
    "    # save plot\n",
    "    if save_file:\n",
    "        save_path = Path(module_path).parent / 'share/' / Path(file_name)\n",
    "        sns.plt.savefig(str(save_path))\n",
    "        \n",
    "    if save_summary:\n",
    "        save_df_to_s3_csv(summary, summary_save_path)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def print_lift_chart_comparison(actual, probability, target, version, num_bins = 10, save_file = False, module_path = None, file_name = None):\n",
    "    # make data\n",
    "    d = {'actual':actual, 'probability':probability}\n",
    "    df = pd.DataFrame(d)\n",
    "    \n",
    "    # get mean probability\n",
    "    mean_probability = df.probability.agg('mean')\n",
    "    mean_actual = df.actual.agg('mean')\n",
    "    \n",
    "    # add bin\n",
    "    df['bins'] = pd.qcut(df.probability, num_bins, duplicates='drop')\n",
    "    \n",
    "    # summarize on bin\n",
    "    summary = df.groupby('bins').agg('mean')\n",
    "    summary['n'] = range(1,len(summary.index)+1)\n",
    "    \n",
    "    # melt\n",
    "    melted_summary = pd.melt(summary, id_vars = 'n', value_vars=['actual', 'probability'])\n",
    "    \n",
    "    # plot comparison\n",
    "    ax = sns.pointplot(x = 'n', y = 'value', hue = 'variable', \n",
    "                       data = melted_summary,\n",
    "                       markers = ['o','o'],\n",
    "                       linestyles = ['--','--']\n",
    "                      )\n",
    "    # set title\n",
    "    ax.set_title('Comparison of actual and probability for SC' + str(target) + ', ' + version)\n",
    "    \n",
    "    # set ylim\n",
    "    ax.set(ylim=(0, None))\n",
    "\n",
    "    # add mean\n",
    "    plt.plot([0,num_bins],[mean_actual,mean_actual], linewidth=2)\n",
    "    \n",
    "    # save plot\n",
    "    if save_file:\n",
    "        save_path = Path(module_path).parent / 'share/' / Path(file_name)\n",
    "        sns.plt.savefig(str(save_path))\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def print_lift_chart_variables(actual, probability, variable, title, num_bins = 10,  save_file = False, module_path = None, file_name = None):\n",
    "    # make data\n",
    "    d = {'actual':actual, 'probability':probability, 'variable':variable}\n",
    "    df = pd.DataFrame(d)\n",
    "    \n",
    "    # get mean probability\n",
    "    mean_actual = df.loc[df.variable == 'test',:].actual.mean()\n",
    "    \n",
    "    # initialzie\n",
    "    temp2 = df.head(0)\n",
    "    \n",
    "    # add bin according to category\n",
    "    for cat in df['variable'].unique():\n",
    "        print(cat)\n",
    "        temp = df.loc[df['variable'] == cat,:].copy()\n",
    "        print(temp.shape)\n",
    "\n",
    "        temp['bins'] = pd.qcut(temp['probability'], num_bins, labels = range(1, num_bins+1))\n",
    "        temp2 = pd.concat([temp2, temp], ignore_index=True)\n",
    "    \n",
    "    df = temp2\n",
    "    \n",
    "    # summarize on bin\n",
    "    summary = df.groupby(['variable', 'bins']).agg('mean')\n",
    "    summary.reset_index(inplace = True)\n",
    "    \n",
    "    # get score\n",
    "    summary_test = summary.loc[summary.variable == 'test', :].copy()\n",
    "    summary_test['score'] = 100 / num_bins * summary_test.bins\n",
    "    score = '{:.2f}'.format(sum(summary_test.actual * summary_test.score) / sum(summary_test.actual))\n",
    "    \n",
    "    # plot\n",
    "    ax = sns.pointplot(x = 'bins', y = 'actual', hue = 'variable', data = summary)\n",
    "\n",
    "    # add title\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # add score\n",
    "    ax.set_xlabel('Score: ' + score)\n",
    "\n",
    "    # add mean\n",
    "    ax.plot([0,num_bins],[mean_actual,mean_actual], linewidth=2)\n",
    "    \n",
    "    # set ylim\n",
    "    ax.set(ylim=(0, None))\n",
    "\n",
    "    # save plot\n",
    "    if save_file:\n",
    "        save_path = Path(module_path).parent / 'share/' / Path(file_name)\n",
    "        sns.plt.savefig(str(save_path))    \n",
    "    \n",
    "    return ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "787px",
    "left": "0px",
    "right": "1708px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
