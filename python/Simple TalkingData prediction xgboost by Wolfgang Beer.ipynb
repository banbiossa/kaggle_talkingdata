{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make the kernel from https://www.kaggle.com/wolfgangb33r/simple-talkingdata-prediction-xgboost/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T05:30:36.862897Z",
     "start_time": "2018-04-24T05:30:34.382799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/kaggle/kaggle_talkingdata\t<<<<<< get to project\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import git\n",
    "from pandas import DataFrame, Series\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "import os\n",
    "import sys\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "module_path = repo.working_tree_dir\n",
    "print(module_path + \"\\t<<<<<< get to project\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "from boto.s3.connection import S3Connection\n",
    "import tarfile\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T05:30:46.492056Z",
     "start_time": "2018-04-24T05:30:46.254581Z"
    }
   },
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import zipfile\n",
    "import subprocess\n",
    "import psutil\n",
    "import base64\n",
    "import zlib\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import dask.bag as db\n",
    "import calendar\n",
    "import glob\n",
    "import time\n",
    "from sys import getsizeof\n",
    "from boto.s3 import key\n",
    "from datetime import datetime,timedelta\n",
    "from boto.s3.key import Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T05:33:07.654265Z",
     "start_time": "2018-04-24T05:33:07.526889Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T05:34:15.097301Z",
     "start_time": "2018-04-24T05:34:15.083048Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'user_defined_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c89cffbd681b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0muser_defined_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebookutil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnbu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotebookFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'user_defined_functions'"
     ]
    }
   ],
   "source": [
    "import user_defined_functions.notebookutil as nbu\n",
    "sys.meta_path.append(nbu.NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import user_defined_functions.helper_functions as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T05:33:46.002962Z",
     "start_time": "2018-04-24T05:33:46.000052Z"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# create a xgboost model\n",
    "model = xgb.XGBClassifier(max_depth=3, n_estimators=600, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already prepared for chunked learning in case that full training data\n",
    "# set does not fit into memory\n",
    "# fit the model\n",
    "chunksize = 4000000 \n",
    "chunk_idx = 0 \n",
    "chunk_max = 1\n",
    "for chunk in pd.read_csv('../input/train.csv', chunksize=chunksize):\n",
    "    # Extracting new features\n",
    "    chunk['hour'] = pd.to_datetime(chunk.click_time).dt.hour.astype('uint8')\n",
    "    chunk['day'] = pd.to_datetime(chunk.click_time).dt.day.astype('uint8')\n",
    "    train_X = chunk.as_matrix(columns=['ip', 'app', 'device', 'os', 'channel', 'hour', 'day'])\n",
    "    if chunk_idx > 0: # not load in first run\n",
    "        model.fit(train_X, chunk['is_attributed'], xgb_model=model)\n",
    "    else:\n",
    "        model.fit(train_X, chunk['is_attributed'])\n",
    "    print(\"Processed chunk %d\" % chunk_idx)\n",
    "    chunk_idx = chunk_idx + 1\n",
    "    if chunk_idx >= chunk_max:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "    # read test data set\n",
    "    test = pd.read_csv('../input/test.csv')\n",
    "    test['hour'] = pd.to_datetime(test.click_time).dt.hour.astype('uint8')\n",
    "    test['day'] = pd.to_datetime(test.click_time).dt.day.astype('uint8')\n",
    "    \n",
    "    test_X = test.as_matrix(columns=['ip', 'app', 'device', 'os', 'channel', 'hour', 'day'])\n",
    "    start_time = print_duration (start_time, \"Finished training, start prediction\")   \n",
    "    # predict the propabilities for binary classes    \n",
    "    pred = model.predict_proba(test_X)\n",
    "    \n",
    "    start_time = print_duration (start_time, \"Finished prediction, start store results\")    \n",
    "    submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "    submission['is_attributed'] = pred[:,1]\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    start_time = print_duration(start_time, \"Finished to store result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
